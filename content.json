{"posts":[{"title":"Hello World","text":"Welcome to my Blog! this is my first Post. My name is Iphigenie Bera and I can’t wait to share my knowledge, learning journey and bits of my life here. I am currently pursuing my Undergraduate degree in Computer Science at Northwestern University with a minor in Data science and Engineering. I was born and raised in Rwanda before coming in the USA for my studies. I am interested in everything data: data engineering, data science, data analytics and software engineering. I believe in the power of data in making sound decisions and optimizing resources. I will post articles about projects with code snippets and demos, if applicable in Projects category. you can look at my resume from navigation bar resume. Feel free to browse my posts by tags or category or track past post in the archives. you can also search for posts containing specific key word from search tool in the navigation bar. you can reach out to me on Github or linkedIn.","link":"/Other/hello-world/"},{"title":"Korean drama webscraping","text":"(on the cover) Hospital Playlist TLDR: I made a list of korean Dramas with rankings and other cool descriptors for each drama IntroductionAs an avid Korean drama watcher, finding the next great drama to watch is the real deal. From my personal experience, most Korean dramas plots are slow-burn meaning that you have to watch multiple episode to understand well the story. For shows which takes more than 10 50-minute-ish episodes, getting a good drama recommendation is a life-saver. When we had a webscraping class during my intro to data science class, I quickly grasped my passion project:Building a giant list of Korean dramas from different websites and then analyse which drama to watch and where. ProcessI collected ratings, platforms, release year, number of episodes, tags,airing times, episode length, description and many more descriptors with the goal of developing robust recommender system for myself. I used Beautifulsoup python library to webscrape Wikipedia, mydramalist, and imdb websites. MyDramalist is a website containing a variety of asian dramas and movies with excellent breadth when it comes to giving drama metadata. The Internet Movie Database (IMDb) is an online database containing information and statistics about movies, TV shows and video games as well as actors, directors and other film industry professionals. Its popularity means it gives credible ratings as it have large user base. PlanningI webscraped Korean drama list from wikipedia gaining about 1500 korean dramas. 1234567891011121314151617181920212223from bs4 import BeautifulSoupimport requestsdef get_wikilinks(): url=&quot;https://en.wikipedia.org/wiki/List_of_South_Korean_dramas&quot; r=requests.get(url) soup=BeautifulSoup(r.content,&quot;html.parser&quot;,from_encoding='utf-8') x=soup.find_all('ul') start=0 movies={} for i in x: if start&gt;=2: break for a in i.find_all('a', href=True): if a['href']==&quot;#See_also&quot;: start+=1 elif a['href']==&quot;/wiki/List_of_South_Korean_television_series&quot;: start+=1 break elif start: movies[a.get_text().strip()]=&quot;https://en.wikipedia.org&quot;+a['href'] return movies code for scrapping list of korean dramas from wikipedia I then used the list to find a breadth of descriptors from mydramalist website through search and scrape strategy getting about 1300 korean dramas. On the side, I webscraped IMDB to get imdb rating and concise drama description, generating about 1510 korean drama. I finally joined one from dramalist and imdb, leading to a final list of about 750 korean drama, this was because imdb list included miniseries or korean drama names differed drastically from names in mydramalist. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# getting dramalist from wikipediawikilinks=get_wikilinks()dramas=wikilinks.keys()##test example (it is important to keep a small list for testing because websites limit number# of requests so limiting webscraping instances is crucial)test=['The Greatest Marriage',&quot;Vincenzo&quot;]#get mydramalist korean dramalistdramalist=get_dramalist(dramas)imdb=get_imdb_ratings()[0]#contains 'Also Known as' section of mydramalist dramalistother_names=dramalist[1]#store other descriptors from mydramalistdrama_descr=dramalist[0]#stores final list of dramaswith_imdb={}for movie in imdb.keys(): for drama in drama_descr.keys(): #check if drama name from imdb is same as name in mydramalist name or is in # 'Also known as' section of mydramalist descriptors if drama in other_names.keys() and (movie.lower()==drama.lower() or (movie.lower() in other_names[drama])): with_imdb[drama]=drama_descr[drama] with_imdb[drama]['imdb_name']=movie with_imdb[drama]['imdb_rating']=imdb[movie][0] with_imdb[drama]['imdb_user_count']=imdb[movie][1] with_imdb[drama]['imdb_description']=imdb[movie][2] ###(testing outputs ongo helps to test data sanity) #print([drama,with_imdb[drama]])for val in with_imdb: #formatting entries with_imdb[val][&quot;Tags&quot;][-1]=with_imdb[val][&quot;Tags&quot;][-1].replace(&quot;(vote or add tags)&quot;,&quot;&quot;) temp=re.findall(&quot;\\d+\\.?\\d*&quot;,with_imdb[drama][&quot;imdb_rating&quot;]) if len(temp)==1: with_imdb[drama][&quot;imdb_rating&quot;]=temp[0]df=pd.DataFrame(with_imdb)df = df.transpose()###storing korean drama list into a pandas dataframedf.to_csv(&quot;kdramalist.csv&quot;, encoding='utf-8', index=False,na_rep=&quot;N/A&quot;)###evaluate accuracyprint(&quot;#############lengths: &quot;,(len(with_imdb),len(drama_descr)))#############lengths: (741, 1279) this dataset can be used for multiple data science projects including sentiment analysis,supervised learning and recommender systems. You can download the dataset from Kaggle and and look at the full code from Github. This Webscraping project is part of Kdrama project in which I practice my data science and engineering skills, step by step until I achieve highly accurate model for recommending korean drama. My milestone projects (not in order) are as below: Data collection Webscraping (done) (read the write above on webscraping project) APIs Datasets ETL process:the goal is to automate data collection data modelling Data pipelining Dashboarding Data visualization and statistics metric analysis more Dashboards visualization Machine learning:(the cool part, you know) machine learning models Recommender system :) Sentiment analysis and NLP Advanced sophisticated machine learning analytics using alternate data and AI computing advancements","link":"/Projects/korean-drama-webscraping/"},{"title":"Pomodoro Timer","text":"A react timer app that uses pomodoro technique. The Pomodoro Technique is a time management method developed by Francesco Cirillo in the late 1980s.[1] It uses a kitchen timer to break work into intervals, typically 25 minutes in length, separated by short breaks. In this project I followed the classic values 25 minutes and 5 minute break as default values and created a pomodoro timer clone using React web. this project was part of final project in FreecodeCamp Front-end Development course, check their cool curriculum here(I am not sponsored, I can vouch for supportive cummunity there). I leveraged scalable vector graphics (svg) its versatility properties in making shapes. the shape and number displays changes accordingly as the countdown starts. the codepen project is embedded below See the Pen pomodoro clock by Iphigenie Bera (@IphixLi) on CodePen.","link":"/Projects/pomodoro-timer/"},{"title":"random quote generator","text":"A react app that generate random quotes from an API as the user clicks on the button. IntroductionReact is one of powerful frontend frameworks when it comes to state management. Developed by Meta (former Facebook), React have an in-built state object which seamlessly manages display of your webapp according to interactions with your application. in this project I used React to generate random quotes from an API as the user clicks on the button. the webapp is hosted for free on heroku.","link":"/Projects/random-quote-generator/"},{"title":"Korean drama Normalization","text":"(on the cover) My Liberation Notes TLDR: I normalized korean drama list dataset for easy querying and accessibility. I created individual genres,tags,platforms,aired_on, and imdb tables to eliminate arrays in table. In RDMBS design, usability plays a big role in data modeling. You have to consider the data stakeholders involved and their use case such as how frequently queries will be performed, new data instances will be inserted, updated or deleted. Data normalization serves this purpose. Nested elements and list elements in RDBMS are notoriously difficult to query. Most of the time, there is not-so-pretty destructuring and joins along the way just to make a simple query on elements with compound data types such as arrays which ends up making querying and updating compound entries take a lot of computing time and resources. Let’s take a look at a part of kdrama list. full kdramalist on kaggle Let’s say you want to access drama names with ‘law’ in genres. The solution would be to check if each entry have ‘law’ in its Genres attribute. Time complexity of this would be O(n^2) which would make even relational data models useless (this is no different than if statements in programming languages like python). Also parsing different file types into sql statements becomes a challenge for compound datatypes such as lists in csv. Csv by default stores elements as strings, so lists become strings, and straight forward queries such as indexing access are a challenge if not even impossible without string manipulation preprocessing. Also for convenience to people who accessesses your date, it is better to keep a table at atomic level for each cell and provide structure for altering and accessing data without necessarily understanding the whole structure of data artifact or tampering data intergrity, this is where data normalization comes into play According to wikipedia, “Database normalization is the process of restructuring a relational database in accordance with a series of so-called normal forms in order to reduce data redundancy and improve data integrity. It was first proposed by Edgar F. Codd as an integral part of his relational model. This leads to the creation of duplicate values and multiple tables. There are many types of normalization but the most commonly used are 3. First normal form (1NF): requires that a table satisfies the following conditions: There is duplicated data: no two rows have same values for each attribute. All columns are “regular” with no hidden values: this means that each cell contains a single value. 1NF normalization created atomic values however can lead to creation of lots of rows as you need to account for possible combinations in case of multiple values destructuring. For example, 1NF of kdramalist table would look like this: Notice how permutations between tags, aired on and actors attributes. While this gives us a single element for each cell, we end up getting a large number of rows. For the kdrama list which had about 743 rows, 1NF led to ~510,000 rows, about 650 times the original list. Imagine if we had an extensive number of tags, genres for each kdrama, we can jump to gigabyte file in instant, oof!!!!. This is not to mention if you need to update an attribute in this kind of table, copying over elements and checking for duplicates would be a nightmare. This is where the second normal form comes for a save. Second normal form (2NF): An entity is in a second normal form if all of its attributes depend on the whole primary key. So this means that the values in the different columns have a dependency on the other columns. The table must be already in 1 NF and all non-key columns of the tables must depend on the PRIMARY KEY. The partial dependencies are removed and placed in a separate table. 2NF strictly guides us to keep only columns which depend on primary keys and keep others in separate tables. This is why we have created additional tables: actors, genres, tags, platforms, aired_on, imdb and one table main_descriptors containing attributes that depend on the primary key, imdb_name. To enforce primary keys on actors, genres, tags, platforms, aired_on, and imdb tables, one can create composite keys using sql. Third normal form (3NF): you should eliminate fields in a table that do not depend on the key. A Table is already in 2 NF Non-Primary key columns shouldn’t depend on the other non-Primary key columns There is no transitive functional dependency. If A-&gt;B and B-&gt;C are the two functional dependencies, then A-&gt;C is called the Transitive Dependency. Our current tables satisfy the above conditions therefore all of them are in 3rd normal form. Main_descriptors actors, genres,tags,platforms,aired_on, and imdb tables can be found in Kaggle. and this github repo under normalized_tables folder and generating code in normalizing.py. you can read on how korean drama dataset was generated by webscraping in this post. This data normalization project is part of Kdrama project in which I practice my data science and engineering skills, step by step until I achieve highly accurate model for recommending korean drama. you can watch projects associated with k-drama projects by following k-drama tag","link":"/Projects/korean-drama-normalization/"}],"tags":[{"name":"Introduction","slug":"Introduction","link":"/tags/Introduction/"},{"name":"Hello World","slug":"Hello-World","link":"/tags/Hello-World/"},{"name":"webscraping","slug":"webscraping","link":"/tags/webscraping/"},{"name":"data engineering","slug":"data-engineering","link":"/tags/data-engineering/"},{"name":"data mining","slug":"data-mining","link":"/tags/data-mining/"},{"name":"k-drama","slug":"k-drama","link":"/tags/k-drama/"},{"name":"BeautifulSoup","slug":"BeautifulSoup","link":"/tags/BeautifulSoup/"},{"name":"Pandas","slug":"Pandas","link":"/tags/Pandas/"},{"name":"React","slug":"React","link":"/tags/React/"},{"name":"codepen","slug":"codepen","link":"/tags/codepen/"},{"name":"heroku","slug":"heroku","link":"/tags/heroku/"},{"name":"Data normalization","slug":"Data-normalization","link":"/tags/Data-normalization/"},{"name":"sql","slug":"sql","link":"/tags/sql/"},{"name":"databases","slug":"databases","link":"/tags/databases/"}],"categories":[{"name":"Other","slug":"Other","link":"/categories/Other/"},{"name":"Projects","slug":"Projects","link":"/categories/Projects/"}],"pages":[{"title":"Introduction","text":"Iphigenie Bera who am IMy name is Iphigenie Bera, and I am an Undergraduate Computer Science student at Northwestern University, USA. I worked at Meta(former Facebook) as data engineering intern and worked as a Research Assistant in a surface engineering lab at Northwestern University. My study and work interests mainly are in the areas of machine learning, artificial intelligence, general computer science, data engineering and software engineering. I am a MOOC courses junkie, so I consume a couple of courses on Coursera, FreecodeCamp, Youtube and Udemy to supplement my college courses. Before joining Computer Science program, I was in Mechanical Engineering program where during first college year I picked up engineering analysis skills and felt in love with programming and decided to join Computer Science, the rest is history. InterestsI am a data-first person. I first ensure data quality of data and identify insights from data. I advocate for responsible data science practices by addressing bias associated with data processes. From Healthcare, infrastructure planning, finance, media and all the way to digital intergration with physical systems,I am interested in design of data intensive systems, enforcing better data governance, agility and resourcefulness. HobbiesI love flowers and wouldn’t be ashamed to take picture of flowers in public [ I might make a big dataset out of my flowers collection, who knows:) ]. I like to watch Korean dramas and listen to BTS, so I am casual Korean learner.","link":"/about/index.html"}]}