{"posts":[{"title":"Hello World","text":"Welcome to my Blog! this is my first Post. My name is Iphigenie Bera and I can’t wait to share my knowledge, learning journey and bits of my life here. I am currently pursuing my Undergraduate degree in Computer Science at Northwestern University with a minor in Data science and Engineering. I was born and raised in Rwanda before coming in the USA for my studies. I am interested in everything data: data engineering, data science, data analytics and software engineering. I believe in the power of data in making sound decisions and optimizing resources. I will post articles about projects with code snippets and demos, if applicable in Projects category. you can look at my resume from navigation bar resume. Feel free to browse my posts by tags or category or track past post in the archives. you can also search for posts containing specific key word from search tool in the navigation bar. you can reach out to me on Github or linkedIn.","link":"/Other/hello-world/"},{"title":"Korean drama Normalization","text":"(on the cover) My Liberation Notes TLDR: I normalized korean drama list dataset for easy querying and accessibility. I created individual genres,tags,platforms,aired_on, and imdb tables to eliminate arrays in table. In RDMBS design, usability plays a big role in data modeling. You have to consider the data stakeholders involved and their use case such as how frequently queries will be performed, new data instances will be inserted, updated or deleted. Data normalization serves this purpose. Nested elements and list elements in RDBMS are notoriously difficult to query. Most of the time, there is not-so-pretty destructuring and joins along the way just to make a simple query on elements with compound data types such as arrays which ends up making querying and updating compound entries take a lot of computing time and resources. Let’s take a look at a part of kdrama list. full kdramalist on kaggle Also parsing different file types into sql statements becomes a challenge for compound datatypes such as lists in csv. Csv by default stores elements as strings, so lists become strings, and straight forward queries such as indexing access are a challenge if not even impossible without string manipulation preprocessing. Also for convenience to people who accessesses your data, it is better to keep a table at atomic level for each cell and provide structure for altering and accessing data without necessarily understanding the whole structure of data artifact or tampering data intergrity, this is where data normalization comes into play According to wikipedia, “Database normalization is the process of restructuring a relational database in accordance with a series of so-called normal forms in order to reduce data redundancy and improve data integrity. It was first proposed by Edgar F. Codd as an integral part of his relational model. This leads to the creation of duplicate values and multiple tables. There are many types of normalization but the most commonly used are 3. First normal form (1NF): requires that a table satisfies the following conditions: There is duplicated data: no two rows have same values for each attribute. All columns are “regular” with no hidden values: this means that each cell contains a single value. 1NF normalization created atomic values however can lead to creation of lots of rows as you need to account for possible combinations in case of multiple values destructuring. For example, 1NF of kdramalist table would look like this: Notice how permutations between tags, aired on and actors attributes. While this gives us a single element for each cell, we end up getting a large number of rows. For the kdrama list which had about 743 rows, 1NF led to ~510,000 rows, about 650 times the original list. Imagine if we had an extensive number of tags, genres for each kdrama, we can jump to gigabyte file in instant, oof!!!!. This is not to mention if you need to update an attribute in this kind of table, copying over elements and checking for duplicates would be a nightmare. This is where the second normal form comes for a save. Second normal form (2NF): An entity is in a second normal form if all of its attributes depend on the whole primary key. So this means that the values in the different columns have a dependency on the other columns. The table must be already in 1 NF and all non-key columns of the tables must depend on the PRIMARY KEY. The partial dependencies are removed and placed in a separate table. 2NF strictly guides us to keep only columns which depend on primary keys and keep others in separate tables. This is why we have created additional tables: actors, genres, tags, platforms, aired_on, imdb and one table main_descriptors containing attributes that depend on the primary key, imdb_name. To enforce primary keys on actors, genres, tags, platforms, aired_on, and imdb tables, one can create composite keys using sql. Third normal form (3NF): you should eliminate fields in a table that do not depend on the key. A Table is already in 2 NF Non-Primary key columns shouldn’t depend on the other non-Primary key columns There is no transitive functional dependency. If A-&gt;B and B-&gt;C are the two functional dependencies, then A-&gt;C is called the Transitive Dependency. Our current tables satisfy the above conditions therefore all of them are in 3rd normal form. Main_descriptors actors, genres,tags,platforms,aired_on, and imdb tables can be found in Kaggle. and this github repo under normalized_tables folder and generating code in normalizing.py. you can read on how korean drama dataset was generated by webscraping in this post. This data normalization project is part of Kdrama project in which I practice my data science and engineering skills, step by step until I achieve highly accurate model for recommending korean drama. you can watch projects associated with k-drama projects by following k-drama tag","link":"/Projects/korean-drama-normalization/"},{"title":"random quote generator","text":"A react app that generate random quotes from an API as the user clicks on the button. IntroductionReact is one of powerful frontend frameworks when it comes to state management. Developed by Meta (former Facebook), React have an in-built state object which seamlessly manages display of your webapp according to interactions with your application. in this project I used React to generate random quotes from an API as the user clicks on the button. the webapp is hosted for free on heroku.","link":"/Projects/random-quote-generator/"},{"title":"Pomodoro Timer","text":"A react timer app that uses pomodoro technique. The Pomodoro Technique is a time management method developed by Francesco Cirillo in the late 1980s.[1] It uses a kitchen timer to break work into intervals, typically 25 minutes in length, separated by short breaks. In this project I followed the classic values 25 minutes and 5 minute break as default values and created a pomodoro timer clone using React web. this project was part of final project in FreecodeCamp Front-end Development course, check their cool curriculum here(I am not sponsored, I can vouch for supportive cummunity there). I leveraged scalable vector graphics (svg) its versatility properties in making shapes. the shape and number displays changes accordingly as the countdown starts. the codepen project is embedded below See the Pen pomodoro clock by Iphigenie Bera (@IphixLi) on CodePen.","link":"/Projects/pomodoro-timer/"},{"title":"Korean drama webscraping","text":"(on the cover) Hospital Playlist TLDR: I made a list of korean Dramas with rankings and other cool descriptors for each drama IntroductionAs an avid Korean drama watcher, finding the next great drama to watch is the real deal. From my personal experience, most Korean dramas plots are slow-burn meaning that you have to watch multiple episode to understand well the story. For shows which takes more than 10 50-minute-ish episodes, getting a good drama recommendation is a life-saver. When we had a webscraping class during my intro to data science class, I quickly grasped my passion project:Building a giant list of Korean dramas from different websites and then analyse which drama to watch and where. ProcessI collected ratings, platforms, release year, number of episodes, tags,airing times, episode length, description and many more descriptors with the goal of developing robust recommender system for myself. I used Beautifulsoup python library to webscrape Wikipedia, mydramalist, and imdb websites. MyDramalist is a website containing a variety of asian dramas and movies with excellent breadth when it comes to giving drama metadata. The Internet Movie Database (IMDb) is an online database containing information and statistics about movies, TV shows and video games as well as actors, directors and other film industry professionals. Its popularity means it gives credible ratings as it have large user base. PlanningI webscraped Korean drama list from wikipedia gaining about 1500 korean dramas. the first step in every webscraping is understanding structur of page source. you can do that by right-click and opening inspect for a localized area of view page source for viewing the whole page. this helps to know how data of interest in encapsuled in html tags and detecting patterns. in wikipedia, list of dramas were in ul tags how to inspect page source 1234567891011121314151617181920212223from bs4 import BeautifulSoupimport requestsdef get_wikilinks(): url=&quot;https://en.wikipedia.org/wiki/List_of_South_Korean_dramas&quot; r=requests.get(url) soup=BeautifulSoup(r.content,&quot;html.parser&quot;,from_encoding='utf-8') x=soup.find_all('ul') start=0 movies={} for i in x: if start&gt;=2: break for a in i.find_all('a', href=True): if a['href']==&quot;#See_also&quot;: start+=1 elif a['href']==&quot;/wiki/List_of_South_Korean_television_series&quot;: start+=1 break elif start: movies[a.get_text().strip()]=&quot;https://en.wikipedia.org&quot;+a['href'] return movies code for scrapping list of korean dramas from wikipedia I then used the list to find a breadth of descriptors from mydramalist website through search and scrape strategy getting about 1300 korean dramas. On the side, I webscraped IMDB to get imdb rating and concise drama description, generating about 1510 korean drama. I finally joined one from dramalist and imdb, leading to a final list of about 750 korean drama, this was because imdb list included miniseries or korean drama names differed drastically from names in mydramalist. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# getting dramalist from wikipediawikilinks=get_wikilinks()dramas=wikilinks.keys()##test example (it is important to keep a small list for testing because websites limit number# of requests so limiting webscraping instances is crucial)test=['The Greatest Marriage',&quot;Vincenzo&quot;]#get mydramalist korean dramalistdramalist=get_dramalist(dramas)imdb=get_imdb_ratings()[0]#contains 'Also Known as' section of mydramalist dramalistother_names=dramalist[1]#store other descriptors from mydramalistdrama_descr=dramalist[0]#stores final list of dramaswith_imdb={}for movie in imdb.keys(): for drama in drama_descr.keys(): #check if drama name from imdb is same as name in mydramalist name or is in # 'Also known as' section of mydramalist descriptors if drama in other_names.keys() and (movie.lower()==drama.lower() or (movie.lower() in other_names[drama])): with_imdb[drama]=drama_descr[drama] with_imdb[drama]['imdb_name']=movie with_imdb[drama]['imdb_rating']=imdb[movie][0] with_imdb[drama]['imdb_user_count']=imdb[movie][1] with_imdb[drama]['imdb_description']=imdb[movie][2] ###(testing outputs ongo helps to test data sanity) #print([drama,with_imdb[drama]])for val in with_imdb: #formatting entries with_imdb[val][&quot;Tags&quot;][-1]=with_imdb[val][&quot;Tags&quot;][-1].replace(&quot;(vote or add tags)&quot;,&quot;&quot;) temp=re.findall(&quot;\\d+\\.?\\d*&quot;,with_imdb[drama][&quot;imdb_rating&quot;]) if len(temp)==1: with_imdb[drama][&quot;imdb_rating&quot;]=temp[0]df=pd.DataFrame(with_imdb)df = df.transpose()###storing korean drama list into a pandas dataframedf.to_csv(&quot;kdramalist.csv&quot;, encoding='utf-8', index=False,na_rep=&quot;N/A&quot;)###evaluate accuracyprint(&quot;#############lengths: &quot;,(len(with_imdb),len(drama_descr)))#############lengths: (741, 1279) this dataset can be used for multiple data science projects including sentiment analysis,supervised learning and recommender systems. You can download the dataset from Kaggle and and look at the full code from Github. This Webscraping project is part of Kdrama project in which I practice my data science and engineering skills, step by step until I achieve highly accurate model for recommending korean drama. My milestone projects (not in order) are as below: Data collection Webscraping (done) (read the write above on webscraping project) APIs Datasets ETL process:the goal is to automate data collection data modelling Data pipelining Dashboarding Data visualization and statistics metric analysis more Dashboards visualization Machine learning:(the cool part, you know) machine learning models Recommender system :) Sentiment analysis and NLP Advanced sophisticated machine learning analytics using alternate data and AI computing advancements","link":"/Projects/korean-drama-webscraping/"},{"title":"Datatalks ML zoomcamp","text":"IntroductionThis machine learning zoomcamp is an online-based machine learning course by Datatalks.Club. it is a learn-by-doing class teaching bread-and-butter skills and techniques in machine learning with projects. The course primarily runs synchronously in cohorts for around 4 months which successful completion of 2 of 3 projects guaranting a certificate. The lecture slides, video and resources are freely available in this course github. I am taking fall cohort 2022 which officially started 5 September 2022. in this post I will post course notes by weeks and links to my homeworks completions. Week 1 - Introduction to Machine Learningintroduction to ML machine learning is the process in the process of extracting patterns from data. machine learning allow us to make models that can classify data given or predict for the future. data can be separated into two types: features : information about the object such as attributes in dataset such as mileage, age, horsepower, location etc. these are the base data for making prediction target: this is the attribute we are developing model to predict for or classify against. for example it can be price of sales prediction. ML vs Rule-Based Systems Rule-Based systems involves manually setting rules and thresholds for prediction and classification however with ever changing data, these systems fails terribly or required a lot of resources and time to adjust to changes. this is where Machine learning come to the save. Machine learning model is trained to find underlying patterns in given data and develop its own thresholds and rules with the help of probability and historical data points. let’s take an example with spam filter. using a rule-based system we can define indicators in the email which can define email as a spam or not such as sender email and email-body contents however sneeky email-sender would quickly bypass this by simply avoiding these rules. with machine learning we can use our training dataset and find out underlying patterns in emails which would suggest if email is spam or not without necessarily knowing them beforehand. types of Machine learning there are two types: Supervised machine learning: they rely on data for which a known target exists (often referred to as labels). these predict output based on training datasets with input-output pairs. unsupervised machine learning/reinforced learning:discover hidden patterns or data groupings without the need of having input-output pairs in training dataset. these fall outside scope of this class. supervised machine learning As said, in supervised machine learning, features are associated with labels. model is trained to associate features with particular labels. this can be done by threshold cutoffs or discrete value association. There is features and targets. features matrix is rows as observations and columns as attributes target matrix: usually a verctor with information we want to predict Types of Supervised ML problems Regression: the output is a number (car’s prize) Classification: the output is a category (spam example). Ranking: the output is the big scores associated with certain items. items are ranked according to their measuring attributes(recommender systems) CRISP-DM The CRoss Industry Standard Process for Data Mining (CRISP-DM) is a methodology for organizing ML projects invented by IBM. Business understanding:do we need ML for the project. are the benefits outweighting costs and uncertainty manageble? Data understanding: Analyze available data sources, and decide if more data is required and transformations that would be needed. Data preparation: Clean data and remove noise applying pipelines, and the data should be converted to a tabular format, so we can put it into ML. Modeling: training data on different models and choose the best one. consider if you would need additional features, data or remove redundant features. Evaluation: Measure how well the model is performing and if it solves the business problem. Deployment: Roll out to production to all the users. The evaluation and deployment often happen together Environment The main programming language used for the course is Python. It is a simple, yet robust language when it comes to handling data. at the time of taking, I used Python 3.9. We used Anaconda python distribution because of benefits of having important data science library we would need; these includes: NumPy: python library for scientific computing expecially with arrays. Pandas: python library for handling tabular data Scikit-Learn:python library for machine-learning models Matplotlib and Seaborn: python library for data visualization Jupyter notebooks: web application for sharing computing documents with input scripting and plain-text capabilities Week 2 - RegressionIntroduction Regression is a supervised machine learning technique. it is used for investigating relation between independent variables and dependent variables. it predicts a dependent variable using a set of indepent variables. in this week, we worked on a project for predicting car prices using regression. dataset used was from this kaggle competition. project plan: Prepare data and Exploratory data analysis (EDA) Validation Framework linear regression Evaluating the model Feature engineering Regularization Using the model Prepare data and Exploratory data analysis (EDA) 1234567#import necessary libraries for EDAimport pandas as pdimport numpy as npimport seaborn as snsfrom matplotlib import pyplot as plt%matplotlib inline read data and preview the first elements in the table 12df = pd.read_csv('data.csv')df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Make Model Year Engine Fuel Type Engine HP Engine Cylinders Transmission Type Driven_Wheels Number of Doors Market Category Vehicle Size Vehicle Style highway MPG city mpg Popularity MSRP 0 BMW 1 Series M 2011 premium unleaded (required) 335.0 6.0 MANUAL rear wheel drive 2.0 Factory Tuner,Luxury,High-Performance Compact Coupe 26 19 3916 46135 1 BMW 1 Series 2011 premium unleaded (required) 300.0 6.0 MANUAL rear wheel drive 2.0 Luxury,Performance Compact Convertible 28 19 3916 40650 2 BMW 1 Series 2011 premium unleaded (required) 300.0 6.0 MANUAL rear wheel drive 2.0 Luxury,High-Performance Compact Coupe 28 20 3916 36350 3 BMW 1 Series 2011 premium unleaded (required) 230.0 6.0 MANUAL rear wheel drive 2.0 Luxury,Performance Compact Coupe 28 18 3916 29450 4 BMW 1 Series 2011 premium unleaded (required) 230.0 6.0 MANUAL rear wheel drive 2.0 Luxury Compact Convertible 28 18 3916 34500 Some table properties 12#get the destribution statistics for numerical columns/attributesdf.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Year Engine HP Engine Cylinders Number of Doors highway MPG city mpg Popularity MSRP count 11914.000000 11845.00000 11884.000000 11908.000000 11914.000000 11914.000000 11914.000000 1.191400e+04 mean 2010.384338 249.38607 5.628829 3.436093 26.637485 19.733255 1554.911197 4.059474e+04 std 7.579740 109.19187 1.780559 0.881315 8.863001 8.987798 1441.855347 6.010910e+04 min 1990.000000 55.00000 0.000000 2.000000 12.000000 7.000000 2.000000 2.000000e+03 25% 2007.000000 170.00000 4.000000 2.000000 22.000000 16.000000 549.000000 2.100000e+04 50% 2015.000000 227.00000 6.000000 4.000000 26.000000 18.000000 1385.000000 2.999500e+04 75% 2016.000000 300.00000 6.000000 4.000000 30.000000 22.000000 2009.000000 4.223125e+04 max 2017.000000 1001.00000 16.000000 4.000000 354.000000 137.000000 5657.000000 2.065902e+06 12#number of entries/data rowsdf.shape[0] 11914 12#number of attributes/columnsdf.shape[1] 16 1234567#formating non-numeral columns to remove spacesdf.columns = df.columns.str.lower().str.replace(' ', '_')string_columns = list(df.dtypes[df.dtypes == 'object'].index)for attr in string_columns: df[attr] = df[attr].str.lower().str.replace(' ', '_') 12345678plt.figure(figsize=(7, 5))sns.histplot(df.msrp, bins=40, alpha=1)plt.ylabel('Frequency')plt.xlabel('Price')plt.title('Distribution of prices')plt.show() distribution of prices, focusing on prices with lower frequencies 12345678plt.figure(figsize=(6, 4))sns.histplot(df.msrp[df.msrp &lt; 100000], bins=40, alpha=1)plt.ylabel('Frequency')plt.xlabel('Price')plt.title('Distribution of prices')plt.show() check for columns with null attributes 1df.isnull().sum() make 0 model 0 year 0 engine_fuel_type 3 engine_hp 69 engine_cylinders 30 transmission_type 0 driven_wheels 0 number_of_doors 6 market_category 3742 vehicle_size 0 vehicle_style 0 highway_mpg 0 city_mpg 0 popularity 0 msrp 0 dtype: int64 Long-tail distributions usually confuse the ML models, so the recommendation is to transform the target variable distribution to a normal one whenever possible. we use log transformation since our data have a long tail suggesting logarithmic distribution 12345678910log_price = np.log1p(df.msrp)plt.figure(figsize=(6, 4))sns.histplot(log_price, bins=40, alpha=1)plt.ylabel('Frequency')plt.xlabel('Log(Price + 1)')plt.title('Distribution of prices after log tranformation')plt.show() Validation Framework The dataset is split into three parts: training, validation, and test. For each partition, we obtain feature matrices (X) and y vectors of targets. The size of partitions is calculated, records are shuffled to guarantee that values of the three partitions contain non-sequential records of the dataset. 12345678910#to allow reproducibility when the code is rerun the second timenp.random.seed(2)n=len(df) #number of rows#performing the split according to ratiosn_val = int(0.2 * n)n_test = int(0.2 * n)n_train = n - (n_val + n_test) 12idx = np.arange(n) #array of numbers from 0 to n 12345#shuffle the number to ensure randomnessnp.random.shuffle(idx)#shuffle tabledf_shuffled = df.iloc[idx] 1234#partitiondf_train = df_shuffled.iloc[:n_train].copy()df_val = df_shuffled.iloc[n_train:n_train+n_val].copy()df_test = df_shuffled.iloc[n_train+n_val:].copy() 12345678910111213y_train_orig = df_train.msrp.valuesy_val_orig = df_val.msrp.valuesy_test_orig = df_test.msrp.values#log transformationy_train = np.log1p(df_train.msrp.values)y_val = np.log1p(df_val.msrp.values)y_test = np.log1p(df_test.msrp.values)del df_train['msrp']del df_val['msrp']del df_test['msrp'] Linear Regression 1234567def train_linear_regression(X,y): ones=np.ones(X.shape[0]) X=np.column_stack([ones,X]) XTX=X.T.dot(X) XTX_inv=np.linalg.inv(XTX) w=XTX_inv.dot(X.T).dot(y) return w[0],w[1:] ######prediction 1234567891011base = ['engine_hp', 'engine_cylinders', 'highway_mpg', 'city_mpg', 'popularity']def prepare_X(df): df_num = df[base] df_num = df_num.fillna(0) X = df_num.values return XX_train = prepare_X(df_train)w_0, w = train_linear_regression(X_train, y_train)print(w_0,w) 7.927257388070001 [ 9.70589522e-03 -1.59103494e-01 1.43792133e-02 1.49441072e-02 -9.06908672e-06] 1234567891011121314#prot for measuring descrepancyy_pred = w_0 + X_train.dot(w)plt.figure(figsize=(6, 4))sns.histplot(y_train, label='target', color='blue', alpha=0.8, bins=40)sns.histplot(y_pred, label='prediction', color='yellow', alpha=0.8, bins=40)plt.legend()plt.ylabel('Frequency')plt.xlabel('Log(Price + 1)')plt.title('Predictions vs actual distribution')plt.show() RMSE Evaluation uses root mean square error to measure efficiency of model. he RMSE represents the differences between predicted values and observed values. the lower the RSME the better the model. 12345def rmse(y, y_pred): error = y_pred - y mse = (error ** 2).mean() return np.sqrt(mse)rmse(y_train, y_pred) 0.7554192603920132 1234#from validation setX_val = prepare_X(df_val)y_pred = w_0 + X_val.dot(w)rmse(y_val, y_pred) 0.7616530991301594 feature engineering feature engineering is manipulating dataset such as addition, deletion, combination, mutation to improve machine learning model training, leading to better performance and greater accuracy.our goal is to have as smaller RSME as possible 1234567891011def prepare_X(df): df = df.copy() features = base.copy() df['age'] = 2017 - df.year features.append('age') df_num = df[features] df_num = df_num.fillna(0) X = df_num.values return X 123456789X_train = prepare_X(df_train)w_0, w = train_linear_regression(X_train, y_train)y_pred = w_0 + X_train.dot(w)print('train', rmse(y_train, y_pred))X_val = prepare_X(df_val)y_pred = w_0 + X_val.dot(w)print('validation', rmse(y_val, y_pred)) train 0.5175055465840046 validation 0.5172055461058324 1df['make'].value_counts().head(5) chevrolet 1123 ford 881 volkswagen 809 toyota 746 dodge 626 Name: make, dtype: int64 12345678910111213141516171819202122top=df['make'].value_counts().head(5)def prepare_X(df): df = df.copy() features = base.copy() df['age'] = 2017 - df.year features.append('age') for v in [2, 3, 4]: feature = 'num_doors_%s' % v df[feature] = (df['number_of_doors'] == v).astype(int) features.append(feature) for v in ['chevrolet', 'ford', 'volkswagen', 'toyota', 'dodge']: feature = 'is_make_%s' % v df[feature] = (df['make'] == v).astype(int) features.append(feature) df_num = df[features] df_num = df_num.fillna(0) X = df_num.values return X 123456789X_train = prepare_X(df_train)w_0, w = train_linear_regression(X_train, y_train)y_pred = w_0 + X_train.dot(w)print('train:', rmse(y_train, y_pred))X_val = prepare_X(df_val)y_pred = w_0 + X_val.dot(w)print('validation:', rmse(y_val, y_pred)) train: 0.5058876515487503 validation: 0.5076038849555671 1234567891011121314151617181920212223242526272829303132def prepare_X(df): df = df.copy() features = base.copy() df['age'] = 2017 - df.year features.append('age') for v in [2, 3, 4]: feature = 'num_doors_%s' % v df[feature] = (df['number_of_doors'] == v).astype(int) features.append(feature) for v in ['chevrolet', 'ford', 'volkswagen', 'toyota', 'dodge']: feature = 'is_make_%s' % v df[feature] = (df['make'] == v).astype(int) features.append(feature) for v in ['regular_unleaded', 'premium_unleaded_(required)', 'premium_unleaded_(recommended)', 'flex-fuel_(unleaded/e85)']: feature = 'is_type_%s' % v df[feature] = (df['engine_fuel_type'] == v).astype(int) features.append(feature) for v in ['automatic', 'manual', 'automated_manual']: feature = 'is_transmission_%s' % v df[feature] = (df['transmission_type'] == v).astype(int) features.append(feature) df_num = df[features] df_num = df_num.fillna(0) X = df_num.values return X 123456789X_train = prepare_X(df_train)w_0, w = train_linear_regression(X_train, y_train)y_pred = w_0 + X_train.dot(w)print('train:', rmse(y_train, y_pred))X_val = prepare_X(df_val)y_pred = w_0 + X_val.dot(w)print('validation:', rmse(y_val, y_pred)) train: 0.4745380510924003 validation: 0.46858791946591755 1df['driven_wheels'].value_counts() front_wheel_drive 4787 rear_wheel_drive 3371 all_wheel_drive 2353 four_wheel_drive 1403 Name: driven_wheels, dtype: int64 12df['market_category'].value_counts().head(5) crossover 1110 flex_fuel 872 luxury 855 luxury,performance 673 hatchback 641 Name: market_category, dtype: int64 1df['vehicle_size'].value_counts().head(5) compact 4764 midsize 4373 large 2777 Name: vehicle_size, dtype: int64 1df['vehicle_size'].value_counts().head(5) compact 4764 midsize 4373 large 2777 Name: vehicle_size, dtype: int64 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253def prepare_X(df): df = df.copy() features = base.copy() df['age'] = 2017 - df.year features.append('age') for v in [2, 3, 4]: feature = 'num_doors_%s' % v df[feature] = (df['number_of_doors'] == v).astype(int) features.append(feature) for v in ['chevrolet', 'ford', 'volkswagen', 'toyota', 'dodge']: feature = 'is_make_%s' % v df[feature] = (df['make'] == v).astype(int) features.append(feature) for v in ['regular_unleaded', 'premium_unleaded_(required)', 'premium_unleaded_(recommended)', 'flex-fuel_(unleaded/e85)']: feature = 'is_type_%s' % v df[feature] = (df['engine_fuel_type'] == v).astype(int) features.append(feature) for v in ['automatic', 'manual', 'automated_manual']: feature = 'is_transmission_%s' % v df[feature] = (df['transmission_type'] == v).astype(int) features.append(feature) for v in ['front_wheel_drive', 'rear_wheel_drive', 'all_wheel_drive', 'four_wheel_drive']: feature = 'is_driven_wheens_%s' % v df[feature] = (df['driven_wheels'] == v).astype(int) features.append(feature) for v in ['crossover', 'flex_fuel', 'luxury', 'luxury,performance', 'hatchback']: feature = 'is_mc_%s' % v df[feature] = (df['market_category'] == v).astype(int) features.append(feature) for v in ['compact', 'midsize', 'large']: feature = 'is_size_%s' % v df[feature] = (df['vehicle_size'] == v).astype(int) features.append(feature) for v in ['sedan', '4dr_suv', 'coupe', 'convertible', '4dr_hatchback']: feature = 'is_style_%s' % v df[feature] = (df['vehicle_style'] == v).astype(int) features.append(feature) df_num = df[features] df_num = df_num.fillna(0) X = df_num.values return X 123456789X_train = prepare_X(df_train)w_0, w = train_linear_regression(X_train, y_train)y_pred = w_0 + X_train.dot(w)print('train:', rmse(y_train, y_pred))X_val = prepare_X(df_val)y_pred = w_0 + X_val.dot(w)print('validation:', rmse(y_val, y_pred)) train: 98.83799994732918 validation: 90.75198555498365 1w_0 -1.1135946518113574e+16 Regularization 1234567891011121314151617def train_linear_regression_reg(X, y, r=0.0): ones = np.ones(X.shape[0]) X = np.column_stack([ones, X]) XTX = X.T.dot(X) reg = r * np.eye(XTX.shape[0]) #print(reg) XTX = XTX + reg XTX_inv = np.linalg.inv(XTX) w = XTX_inv.dot(X.T).dot(y) return w[0], w[1:]X_train = prepare_X(df_train)for r in [0, 0.001, 0.01, 0.1, 1, 10]: w_0, w = train_linear_regression_reg(X_train, y_train, r=r) print('%5s, %.2f, %.2f, %.2f' % (r, w_0, w[13], w[21])) 0, -11135946518113574.00, 12.19, 11135946518113572.00 0.001, 7.20, -0.10, 1.81 0.01, 7.18, -0.10, 1.81 0.1, 7.05, -0.10, 1.78 1, 6.22, -0.10, 1.56 10, 4.39, -0.09, 1.08 123456789X_train = prepare_X(df_train)w_0, w = train_linear_regression_reg(X_train, y_train, r=0)y_pred = w_0 + X_train.dot(w)print('train', rmse(y_train, y_pred))X_val = prepare_X(df_val)y_pred = w_0 + X_val.dot(w)print('val', rmse(y_val, y_pred)) train 98.83799994732918 val 90.75198555498365 1234567X_train = prepare_X(df_train)X_val = prepare_X(df_val)for r in [0.000001, 0.0001, 0.001, 0.01, 0.1, 1, 5, 10]: w_0, w = train_linear_regression_reg(X_train, y_train, r=r) y_pred = w_0 + X_val.dot(w) print('%6s' %r, rmse(y_val, y_pred)) 1e-06 0.46022485228775944 0.0001 0.4602254918041578 0.001 0.4602267630259776 0.01 0.460239496285643 0.1 0.4603700695839839 1 0.4618298042649753 5 0.4684079627532594 10 0.4757248100693656 12345678910X_train = prepare_X(df_train)w_0, w = train_linear_regression_reg(X_train, y_train, r=0.01)X_val = prepare_X(df_val)y_pred = w_0 + X_val.dot(w)print('validation:', rmse(y_val, y_pred))X_test = prepare_X(df_test)y_pred = w_0 + X_test.dot(w)print('test:', rmse(y_test, y_pred)) validation: 0.460239496285643 test: 0.4571813679219644 Using the model 123i = 2ad = df_test.iloc[i].to_dict()ad {'make': 'toyota', 'model': 'venza', 'year': 2013, 'engine_fuel_type': 'regular_unleaded', 'engine_hp': 268.0, 'engine_cylinders': 6.0, 'transmission_type': 'automatic', 'driven_wheels': 'all_wheel_drive', 'number_of_doors': 4.0, 'market_category': 'crossover,performance', 'vehicle_size': 'midsize', 'vehicle_style': 'wagon', 'highway_mpg': 25, 'city_mpg': 18, 'popularity': 2031} X_test = prepare_X(pd.DataFrame([ad]))[0] y_pred = w_0 + X_test.dot(w) suggestion = np.expm1(y_pred) #this is to ensure greater precision than exp(x) - 1 for small values of x suggestion","link":"/Learning/datatalks-ml-zoomcamp/"}],"tags":[{"name":"Introduction","slug":"Introduction","link":"/tags/Introduction/"},{"name":"Hello World","slug":"Hello-World","link":"/tags/Hello-World/"},{"name":"Data normalization","slug":"Data-normalization","link":"/tags/Data-normalization/"},{"name":"sql","slug":"sql","link":"/tags/sql/"},{"name":"databases","slug":"databases","link":"/tags/databases/"},{"name":"data engineering","slug":"data-engineering","link":"/tags/data-engineering/"},{"name":"k-drama","slug":"k-drama","link":"/tags/k-drama/"},{"name":"Pandas","slug":"Pandas","link":"/tags/Pandas/"},{"name":"React","slug":"React","link":"/tags/React/"},{"name":"heroku","slug":"heroku","link":"/tags/heroku/"},{"name":"codepen","slug":"codepen","link":"/tags/codepen/"},{"name":"webscraping","slug":"webscraping","link":"/tags/webscraping/"},{"name":"data mining","slug":"data-mining","link":"/tags/data-mining/"},{"name":"BeautifulSoup","slug":"BeautifulSoup","link":"/tags/BeautifulSoup/"},{"name":"Machine learning","slug":"Machine-learning","link":"/tags/Machine-learning/"},{"name":"Datatalks","slug":"Datatalks","link":"/tags/Datatalks/"},{"name":"study-notes","slug":"study-notes","link":"/tags/study-notes/"}],"categories":[{"name":"Other","slug":"Other","link":"/categories/Other/"},{"name":"Projects","slug":"Projects","link":"/categories/Projects/"},{"name":"Learning","slug":"Learning","link":"/categories/Learning/"}],"pages":[{"title":"Introduction","text":"Iphigenie Bera who am IMy name is Iphigenie Bera, and I am an Undergraduate Computer Science student at Northwestern University, USA. I worked at Meta(former Facebook) as data engineering intern and worked as a Research Assistant in a surface engineering lab at Northwestern University. My study and work interests mainly are in the areas of machine learning, artificial intelligence, general computer science, data engineering and software engineering. I am a MOOC courses junkie, so I consume a couple of courses on Coursera, FreecodeCamp, Youtube and Udemy to supplement my college courses. Before joining Computer Science program, I was in Mechanical Engineering program where during first college year I picked up engineering analysis skills and felt in love with programming and decided to join Computer Science, the rest is history. InterestsI am a data-first person. I first ensure data quality of data and identify insights from data. I advocate for responsible data science practices by addressing bias associated with data processes. From Healthcare, infrastructure planning, finance, media and all the way to digital intergration with physical systems,I am interested in design of data intensive systems, enforcing better data governance, agility and resourcefulness. HobbiesI love flowers and wouldn’t be ashamed to take picture of flowers in public [ I might make a big dataset out of my flowers collection, who knows:) ]. I like to watch Korean dramas and listen to BTS, so I am casual Korean learner.","link":"/about/index.html"}]}